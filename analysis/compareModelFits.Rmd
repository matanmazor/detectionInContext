---
title: "Drift diffusion modelling of visual detection"
author: "Matan Mazor"
date: "2023-04-04"
output: word_document
---

```{r setup, include=FALSE}
library('groundhog')
groundhog.library(
  c(
    'papaja',
    'reticulate',
    'tidyverse',
    'broom',
    'cowplot',
    'MESS', # for AUCs
    'lsr', # for effect sizes
    'pwr', # for power calculations
    'brms', # for mixed effects modeling
    'BayesFactor',# for Bayesian t test
    'jsonlite', #parsing data from sort_trial
    'afex' #for anova
  ), "2022-12-01"
)
r_refs("r-references.bib")
knitr::opts_chunk$set(echo=F,message=F, warning=F)

```

## The models

First, we defined a family of models, based on the standard drift diffusion modelling (DDM) framework:

1.  **A standard symmetric model**. In this model, boundary separation and starting point are allowed to vary. Drift rate is also allowed to vary between target-present and target-absent responses.
2.  **An asymmetric model with linearly collapsing boundaries**. In this model, boundary separation and starting point are allowed to vary. Unlike model 1, here drift rate is set to 0 in target-absent trials, and can vary only in target-present trials. The two boundaries are independently collapsing at a constant speed that is controlled by two additional free parameters.
3.  **An asymmetric model with a non-linearly collapsing boundary.** Similar to model 2, but here only the lower boundary collapses. Furthermore, collapse starts after a fixed delay. With high collapse rates, this can be used as an effective "timer": decisions about absence are made after a fixed amount of time, given the accumulator did not reach the upper boundary.
4.  **A standard model with unequal variance**. Similar to model 1, but drift noise is allowed to vary between target-present and target-absent trials.
5.  **An asymmetric model with a linearly collapsing boundary and unequal variance.** Similar to model 2, but drift noise is allowed to vary between target-present and target-absent trials.
6.  **An asymmetric model with a non-linearly collapsing boundary and unequal variance**. Similar to model 3, but drift noise is allowed to vary between target-present and target-absent trials.

## Model recovery

```{r model_recovery, fig.cap="Model recovery. Presenting the relative BIC for all 36 combinations of true and fitted models. Good model recovery is indicated by overall dark (negative) values along the main diagonal."}
fit_df = data.frame();
for (m_model in 1:6) {
  for (m_data in 1:6) {
    fit_df <- fit_df %>% rbind(
      read.csv(paste('../modelling/model_fits/model_m', 
                            as.character(m_model), '/data_m', 
                            as.character(m_data), '/all_subjects.csv', sep='')) %>%
      dplyr::select(subj_id,BIC) %>%
      mutate(model=m_model,data=m_data)
    )
  }
}

fit_df <- fit_df %>% 
  mutate(diagonal = ifelse(data==model,1,0))

fit_df %>%
  group_by(data,subj_id)%>%
  mutate(relative_BIC=BIC-mean(BIC)) %>%
  group_by(data,model)%>%
  summarize(relative_BIC=mean(relative_BIC)) %>%
  ggplot(aes(x=model,y=data,fill=relative_BIC)) +
  geom_tile() +
  scale_x_continuous(breaks=seq(6)) +
  # scale_y_continuous(breaks=seq(6)) +
  scale_y_reverse(breaks=seq(6))
```

From each model, we simulated data from 200 agents, with 64 trials per agent. Agent-level parameters were sampled from population-level distributions, chosen to produce similar levels of accuracy, bias and reaction time as empirical data. Full details are available in the `model-recovery.ipynb` notebook.

We then fit the six models to the six simulated datasets (36 model fits overall), and extracted an adjusted Bayesian Information Criterion (BIC) per subject. The mean BIC was taken as an index of model fit quality, with lower values indicating a better fit. Overall, the true data-generating models were preferred over alternative models (`r apa_print(t.test(fit_df%>%filter(diagonal==1)%>%pull(BIC), fit_df%>%filter(diagonal==0)%>%pull(BIC)))$statistic`). All models were successfully recovered, with the exception of models 4 and 6, which could not be distinguished from their corresponding equal-variance variants. 

Specifically, for data generated from model 1, model 1 was preferred over model 2 (`r apa_print(t.test(fit_df%>%filter(model==1 & data==1)%>%pull(BIC)-fit_df%>%filter(model==2 & data==1)%>%pull(BIC)))$statistic`), 3 (`r apa_print(t.test(fit_df%>%filter(model==1 & data==1)%>%pull(BIC)-fit_df%>%filter(model==3 & data==1)%>%pull(BIC)))$statistic`), 4 (`r apa_print(t.test(fit_df%>%filter(model==1 & data==1)%>%pull(BIC)-fit_df%>%filter(model==4 & data==1)%>%pull(BIC)))$statistic`), 5 (`r apa_print(t.test(fit_df%>%filter(model==1 & data==1)%>%pull(BIC)-fit_df%>%filter(model==5 & data==1)%>%pull(BIC)))$statistic`) and 6 (`r apa_print(t.test(fit_df%>%filter(model==1 & data==1)%>%pull(BIC)-fit_df%>%filter(model==6 & data==1)%>%pull(BIC)))$statistic`). 

For data generated from model 2, model 2 was preferred over models 1 (`r apa_print(t.test(fit_df%>%filter(model==2 & data==2)%>%pull(BIC)-fit_df%>%filter(model==1 & data==2)%>%pull(BIC)))$statistic`), 3 (`r apa_print(t.test(fit_df%>%filter(model==2 & data==2)%>%pull(BIC)-fit_df%>%filter(model==3 & data==2)%>%pull(BIC)))$statistic`), 4 (`r apa_print(t.test(fit_df%>%filter(model==2 & data==2)%>%pull(BIC)-fit_df%>%filter(model==4 & data==2)%>%pull(BIC)))$statistic`), 5 (`r apa_print(t.test(fit_df%>%filter(model==2 & data==2)%>%pull(BIC)-fit_df%>%filter(model==5 & data==2)%>%pull(BIC)))$statistic`), and 6 (`r apa_print(t.test(fit_df%>%filter(model==2 & data==2)%>%pull(BIC)-fit_df%>%filter(model==6 & data==2)%>%pull(BIC)))$statistic`).

For data generated from model 3, model 3 was preferred over models 1 (`r apa_print(t.test(fit_df%>%filter(model==3 & data==3)%>%pull(BIC)-fit_df%>%filter(model==1 & data==3)%>%pull(BIC)))$statistic`), 2 (`r apa_print(t.test(fit_df%>%filter(model==3 & data==3)%>%pull(BIC)-fit_df%>%filter(model==2 & data==3)%>%pull(BIC)))$statistic`), 4 (`r apa_print(t.test(fit_df%>%filter(model==3 & data==3)%>%pull(BIC)-fit_df%>%filter(model==4 & data==3)%>%pull(BIC)))$statistic`), 5 (`r apa_print(t.test(fit_df%>%filter(model==3 & data==3)%>%pull(BIC)-fit_df%>%filter(model==5 & data==3)%>%pull(BIC)))$statistic`), and 6 (`r apa_print(t.test(fit_df%>%filter(model==3 & data==3)%>%pull(BIC)-fit_df%>%filter(model==6 & data==3)%>%pull(BIC)))$statistic`).

For data generated from the unequal-variance model 4, model 4 was *not* preferred over its corresponding equal-variance variant model 1 (`r apa_print(t.test(fit_df%>%filter(model==4 & data==4)%>%pull(BIC)-fit_df%>%filter(model==1 & data==4)%>%pull(BIC)))$statistic`), but was preferred over models 2 (`r apa_print(t.test(fit_df%>%filter(model==4 & data==4)%>%pull(BIC)-fit_df%>%filter(model==2 & data==4)%>%pull(BIC)))$statistic`), 3 (`r apa_print(t.test(fit_df%>%filter(model==4 & data==4)%>%pull(BIC)-fit_df%>%filter(model==3 & data==4)%>%pull(BIC)))$statistic`), 5 (`r apa_print(t.test(fit_df%>%filter(model==4 & data==4)%>%pull(BIC)-fit_df%>%filter(model==5 & data==4)%>%pull(BIC)))$statistic`), and 6 (`r apa_print(t.test(fit_df%>%filter(model==4 & data==4)%>%pull(BIC)-fit_df%>%filter(model==6 & data==4)%>%pull(BIC)))$statistic`).

Conversely, for data generated from the unequal-variance model 5, model 5 was preferred over its corresponding equal-variance variant model 2 (`r apa_print(t.test(fit_df%>%filter(model==5 & data==5)%>%pull(BIC)-fit_df%>%filter(model==2 & data==5)%>%pull(BIC)))$statistic`), as well as over models 1 (`r apa_print(t.test(fit_df%>%filter(model==5 & data==5)%>%pull(BIC)-fit_df%>%filter(model==1 & data==5)%>%pull(BIC)))$statistic`), 3 (`r apa_print(t.test(fit_df%>%filter(model==5 & data==5)%>%pull(BIC)-fit_df%>%filter(model==3 & data==5)%>%pull(BIC)))$statistic`), 4 (`r apa_print(t.test(fit_df%>%filter(model==5 & data==5)%>%pull(BIC)-fit_df%>%filter(model==4 & data==5)%>%pull(BIC)))$statistic`), and 6 (`r apa_print(t.test(fit_df%>%filter(model==5 & data==5)%>%pull(BIC)-fit_df%>%filter(model==6 & data==5)%>%pull(BIC)))$statistic`).

Finally, for data generated from the unequal-variance model 6, model 6 was *not* preferred over its corresponding equal-variance variant model 3 (`r apa_print(t.test(fit_df%>%filter(model==6 & data==6)%>%pull(BIC)-fit_df%>%filter(model==3 & data==6)%>%pull(BIC)))$statistic`), but was preferred over models 1 (`r apa_print(t.test(fit_df%>%filter(model==6 & data==6)%>%pull(BIC)-fit_df%>%filter(model==1 & data==6)%>%pull(BIC)))$statistic`), 2 (`r apa_print(t.test(fit_df%>%filter(model==6 & data==6)%>%pull(BIC)-fit_df%>%filter(model==2 & data==6)%>%pull(BIC)))$statistic`), 4 (`r apa_print(t.test(fit_df%>%filter(model==6 & data==6)%>%pull(BIC)-fit_df%>%filter(model==4 & data==6)%>%pull(BIC)))$statistic`), and 5 (`r apa_print(t.test(fit_df%>%filter(model==6 & data==6)%>%pull(BIC)-fit_df%>%filter(model==5 & data==6)%>%pull(BIC)))$statistic`).

## Parameter recovery


```{r parameter_recovery}

true_parameters <- list()
fitted_parameters <- list()

for (i in 1:6) {
  true_parameters[[i]] <- read.csv(paste('../modelling/data/generated_data/m',
                          as.character(i), '_parameters.csv', sep=''))
  fitted_parameters[[i]] <- read.csv(paste('../modelling/model_fits/model_m', 
                          as.character(i), '/data_m', 
                          as.character(i), '/all_subjects.csv', sep='')) 
}

```

In our simulations, model parameters varied between simulated agents. This allowed us to measure not only whether the true model can be recovered, but also our ability to estimate model parameters relative to the ground-truth data-generating model. Given the relatively low number of trials per agent (64), reliably recovering model parameters is not trivial. Still, parameters were successfully recovered. For illustration, we list the correlations between true and recovered key model parameters. 

For model 1, drift rate was successfully recovered (`r apa_print(cor.test(true_parameters[[1]]$v_present, fitted_parameters[[1]]$driftpresent))$full_result`), as was the starting point bias (`r apa_print(cor.test(true_parameters[[1]]$z, fitted_parameters[[1]]$IC))$full_result`) and boundary separation (`r apa_print(cor.test(true_parameters[[1]]$a, fitted_parameters[[1]]$bound))$full_result`). For model 2 and 3, boundary collapse speed was recovered (`r apa_print(cor.test(true_parameters[[2]]$cr_present, fitted_parameters[[2]]$uppercollapse))$full_result` and `r apa_print(cor.test(true_parameters[[3]]$cr_absent, fitted_parameters[[3]]$lowercollapse))$full_result`). For model 3, collapse time was recovered (`r apa_print(cor.test(true_parameters[[3]]$ct_absent, fitted_parameters[[3]]$collapsetime))$full_result`). For model 4, drift noise was successfully recovered (`r apa_print(cor.test(true_parameters[[4]]$sigma_absent, fitted_parameters[[4]]$noiseabsent))$full_result`). 

## Model fit

Reassured by these model and parameter recovery results, we turned to empirical data from Experiments 2-4. We decided not to use data from Exp. 1, as in this experiment visibility gradually increased, violating model assumptions regarding the stability of drift rate over time. 

```{r model_fit, fig.cap="Model fit results for experiments 2-4. Darker values correspond to better fits."}

exp_fit_df = data.frame();
for (m_model in 1:6) {
  for (exp_data in 2:4) {
    exp_fit_df <- exp_fit_df %>% rbind(
      read.csv(paste('../modelling/model_fits/model_m', 
                            as.character(m_model), '/data_E', 
                            as.character(exp_data), '/all_subjects.csv', sep='')) %>%
      dplyr::select(subj_id,BIC) %>%
      mutate(model=m_model,data=exp_data)
    )
  }
}

exp2_m1_fit <- read.csv(paste('../modelling/model_fits/model_m1/data_E2/all_subjects.csv', sep=''));

exp2_m4_fit <- read.csv(paste('../modelling/model_fits/model_m4/data_E2/all_subjects.csv', sep=''));

exp3_m1_fit <- read.csv(paste('../modelling/model_fits/model_m1/data_E3/all_subjects.csv', sep=''));

exp3_m4_fit <- read.csv(paste('../modelling/model_fits/model_m4/data_E3/all_subjects.csv', sep=''));

exp4_m1_fit <- read.csv(paste('../modelling/model_fits/model_m1/data_E4/all_subjects.csv', sep=''));

exp4_m2_fit <- read.csv(paste('../modelling/model_fits/model_m2/data_E4/all_subjects.csv', sep=''));

exp4_m3_fit <- read.csv(paste('../modelling/model_fits/model_m3/data_E4/all_subjects.csv', sep=''));

exp4_m5_fit <- read.csv(paste('../modelling/model_fits/model_m5/data_E4/all_subjects.csv', sep=''));

exp_fit_df %>%
  group_by(data,subj_id)%>%
  mutate(relative_BIC=BIC-mean(BIC)) %>%
  group_by(data,model)%>%
  summarize(relative_BIC=mean(relative_BIC)) %>%
  ggplot(aes(x=model,y=data,fill=relative_BIC)) +
  geom_tile() +
  scale_x_continuous(breaks=seq(6)) +
  scale_y_reverse()+
  labs(y='experiment')
```

### Experiment 2: context effects

In Exp. 2, the target stimulus was flankered by congruent or incongruent letters. For the purpose of this original model selection stage, we collapsed over all trials and asked which model fit the data best. 

Symmetric model 1 fit the data best (mean adjusted BIC =  `r exp_fit_df%>%filter(data==2 & model==1)%>%pull(BIC)%>%mean()%>%printnum()`), significantly better than models 2 (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==2)%>%pull(BIC)-exp_fit_df%>%filter(model==2 & data==2)%>%pull(BIC)))$statistic`), 3 (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==2)%>%pull(BIC)-exp_fit_df%>%filter(model==3 & data==2)%>%pull(BIC)))$statistic`) and the unequal variance variants 4 (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==2)%>%pull(BIC)-exp_fit_df%>%filter(model==4 & data==2)%>%pull(BIC)))$statistic`), 5 (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==2)%>%pull(BIC)-exp_fit_df%>%filter(model==5 & data==2)%>%pull(BIC)))$statistic`) and 6  (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==2)%>%pull(BIC)-exp_fit_df%>%filter(model==6 & data==2)%>%pull(BIC)))$statistic`). 

Together, data from Exp. 2 was best described by a model that assumes evidence accumulation for both presence and absence, with no difference in the associated noise. Interestingly, drift rates in the fitted models were significantly *steeper* in target-absent compared to target-present trials (`r apa_print(t.test(exp2_m1_fit$driftabsent+exp2_m1_fit$driftpresent))$full_result`). This was balanced by a starting point bias that was closer to the upper boundary (`r apa_print(t.test(exp2_m1_fit$IC))$full_result`).

When allowed to vary, drift noise in target-absent trials was significantly reduced relative to target-present trials (`r apa_print(exp2_m4_fit$noiseabsent%>%t.test(mu=1))$full_result` for a t-test against 1). However, releasing the equal variance assumption did not significantly improve model fit, as indicated by the lower BIC for model 1 compared to model 4. 

### Experiment 3: dusty windows

In Exp. 3, the target stimulus was partly occluded by black pixels. Here too, for the purpose of this original model selection stage, we collapsed over all trials and asked which model fit the data best. 

Like in Exp. 2, symmetric model 1 fit the data best (mean adjusted BIC =  `r exp_fit_df%>%filter(data==3 & model==1)%>%pull(BIC)%>%mean()%>%printnum()`), significantly better than models 2 (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==3)%>%pull(BIC)-exp_fit_df%>%filter(model==2 & data==3)%>%pull(BIC)))$statistic`), 3 (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==3)%>%pull(BIC)-exp_fit_df%>%filter(model==3 & data==3)%>%pull(BIC)))$statistic`) and the unequal variance variants 4 (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==3)%>%pull(BIC)-exp_fit_df%>%filter(model==4 & data==3)%>%pull(BIC)))$statistic`), 5 (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==3)%>%pull(BIC)-exp_fit_df%>%filter(model==5 & data==3)%>%pull(BIC)))$statistic`) and 6  (`r apa_print(t.test(exp_fit_df%>%filter(model==1 & data==3)%>%pull(BIC)-exp_fit_df%>%filter(model==6 & data==3)%>%pull(BIC)))$statistic`). 

Mirroring the results from Exp. 2, data was best captured by a symmetric model with equal variance. Here too, drift rates in the fitted models were significantly steeper in target-absent compared to target-present trials (`r apa_print(t.test(exp3_m1_fit$driftabsent+exp3_m1_fit$driftpresent))$full_result`). Again, this was balanced by a starting point bias that was closer to the upper boundary (`r apa_print(t.test(exp3_m1_fit$IC))$full_result`). Finally, despite a preference for model 1, when allowed to vary between target-absent and target-absent trials, drift noise was significantly reduced in target-absent trials (`r apa_print(exp3_m4_fit$noiseabsent%>%t.test(mu=1))$full_result` for a t-test against 1).

### Experiment 4: line occluders

In Exp. 4, stimuli were partly occluded by black horizontal bars. Here too, we collapsed over low and high occlusion levels for the purpose of initial model selection. 

Unlike Exp. 2 and 3, here the best performing model was model 2, where drift rate was set to 0 in target-absent trials, and the lower boundary linearly collapsed  (mean adjusted BIC =  `r exp_fit_df%>%filter(data==4 & model==1)%>%pull(BIC)%>%mean()%>%printnum()`). Model 2 captured the data significantly better than model 1 (`r apa_print(t.test(exp_fit_df%>%filter(model==2 & data==4)%>%pull(BIC)-exp_fit_df%>%filter(model==1 & data==4)%>%pull(BIC)))$statistic`), 4 (`r apa_print(t.test(exp_fit_df%>%filter(model==2 & data==4)%>%pull(BIC)-exp_fit_df%>%filter(model==4 & data==4)%>%pull(BIC)))$statistic`), and 6 (`r apa_print(t.test(exp_fit_df%>%filter(model==2 & data==4)%>%pull(BIC)-exp_fit_df%>%filter(model==6 & data==4)%>%pull(BIC)))$statistic`), and marginally over its corresponding unequal-variance variant model 5 (`r apa_print(t.test(exp_fit_df%>%filter(model==2 & data==4)%>%pull(BIC)-exp_fit_df%>%filter(model==5 & data==4)%>%pull(BIC)))$statistic`). BIC scores for model 2 were not significantly lower than for model 3, where boundary collapse time was allowed to vary (`r apa_print(t.test(exp_fit_df%>%filter(model==2 & data==4)%>%pull(BIC)-exp_fit_df%>%filter(model==3 & data==4)%>%pull(BIC)))$statistic`).

In model 2, the upper and lower boundaries are allowed to collapse. As expected, the lower boundary collapsed much more than the upper boundary in the fitted models (`r apa_print(t.test(exp4_m2_fit$lowercollapse+exp4_m2_fit$uppercollapse))$full_result`). In model 3, the lower boundary can start collapsing after a fixed time. This was estimated as `r exp4_m3_fit$collapsetime%>%mean()%>%printnum()` seconds on average. Again, when allowed to vary between conditions, drift noise was significantly lower in target-absent trials (`r apa_print(t.test(exp4_m5_fit$noiseabsent, mu=1))$statistic`). 

